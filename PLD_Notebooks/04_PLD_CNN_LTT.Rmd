# Importing Libraries and Sources




```{r, setup, include=FALSE}


#workingdirectory
path_dir = "/Users/oasc/Documents/Thesis/ML_phylogeny_learning"

knitr::opts_knit$set(root.dir =path_dir)
```


```{R}
library(torch)
library(luz)



source( "R/neural-network-functions.R")
source( "R/infer-general-functions.R")
source( "R/phylo-inference-ml.R")
source( "R/new_funcs.R")


```



```{R,Tree params}
# Set parameters DDD model simulation

n_trees <- 10000# number of trees to generate
model   <- "PLD"

device <- "cpu"
nn_type <- "cnn-ltt"
#lambda0 <- c(.5, 1.5) # speciation rate
#mu      <- c(0.05,0.5) #  extinction rate
#k	<-c(10,20)         # carrying Capacity
#k	<-c(40,100) 
 
#crown_age <- 1 
dd_model = 1 # dd_model = 1 linear dependence in speciation rate with parameter K (= diversity where speciation = extinction)

path_ddd<-"data_PLD/"
#sumstat_f<-"phylogeny-DDD2-nt-10000-la0-0-50-mu-0-50-k-20-400-age-1-ddmod-10-sumstat.rds"
phylo_f<-"phylogeny-pld-nt-10000-la0-0-50-mu-0-50-k-20-400-age-1-ddmod-10.rds"
true_f<-"true-param-pld-nt-10000-la0-0-50-mu-0-50-k-20-400-age-1-ddmod-10.rds"

```



```{R}


set.seed(113)

#phylo_name<-fname_ddd(n_trees,lambda0,mu,k,crown_age,dd_model)
#if( file.exists( paste("data/phylogeny-",model,"-",phylo_name,".rds",sep="") )==TRUE){

#phylo <- readRDS( paste("data_DDD/phylogeny-",model,"-",phylo_name,".rds",sep=""))
#true <- readRDS(paste("data_/true-param-", model, "-",phylo_name,".rds", sep=""))
#sumstat <- readRDS(paste("data/phylogeny-",model,"-",phylo_name,"-sumstat.rds",sep=""))


phylo <- readRDS( paste(path_ddd,phylo_f,sep=""))
true <- readRDS( paste(path_ddd,true_f,sep=""))

#true[[4]]<-true[[4]]/100 #adjusting parameters

#true<-true[c(-4)] # Removing crown age from variables
true$beta_n <- (true$mu-true$lambda0)/true$K

true<-true[c(1,2,5,4)]

true[[1]]<-true[[1]]/10
true[[2]]<-true[[2]]/10
```





```{R}

max_nodes <- 0  # Variable to store the maximum number of nodes

# Iterate over each tree in the list
for (i in seq_along(phylo)) {
  num_nodes <- phylo[[i]]$Nnode  # Get the number of nodes for the current tree
  max_nodes <- max(max_nodes, num_nodes)  # Update the maximum number of nodes if necessary
}
max_nodes_rounded <- ceiling(max_nodes / 50) * 50
```

```{R}

#phylo_name<-fname_ddd(n_trees,lambda0,mu,k,crown_age,dd_model)


if( file.exists(  paste(path_ddd,"phylo_DDD_dfltt.rds", sep="")) ==FALSE){
start_time <- Sys.time()

df.ltt <- generate_ltt_dataframe(phylo, max_nodes_rounded, true)$ltt
end_time <- Sys.time()
print(end_time - start_time)

saveRDS(df.ltt,  paste(path_ddd,"phylo_DDD_dfltt.rds", sep=""))
#additional tree info


}else{
  print(paste("LTT info from model: ",model,", already generated. Reading file",sep=""))
  
  df.ltt <- readRDS(paste(path_ddd,"phylo_DDD_dfltt.rds", sep=""))
}


```

```{R}
# Parameters of the NN's training

set.seed(113)
total_data_points<-n_trees
subset_size <- 10000  # Specify the size of the subset

n_train    <- floor(subset_size * .9)
n_valid    <- floor(subset_size * .05)
n_test     <- subset_size - n_train - n_valid
#batch_size <- batch_size <- min(subset_size*.01, 10)
batch_size <- batch_size <- 32

# Pick the phylogenies randomly.
ds.ltt <- convert_ltt_dataframe_to_dataset(df.ltt, true, nn_type)


# Pick the random subset of data points.
subset_indices <- sample(1:total_data_points, subset_size)

# Split the subset into train, validation, and test indices.
train_indices <- subset_indices[1:n_train]
valid_indices <- subset_indices[(n_train + 1):(n_train + n_valid)]
test_indices  <- subset_indices[(n_train + n_valid + 1):subset_size]


#n_train    <- .9 *n_trees
#n_valid    <- .05*n_trees
#n_test     <- n_trees - n_train - n_valid
#n_epochs   <- 100
#batch_size <- 64
#patience   <- 10

# Creation of the train, valid and test dataset
#train_indices     <- sample(1:n_trees, n_train)
#not_train_indices <- setdiff(1:n_trees, train_indices)
#valid_indices     <- sample(not_train_indices, n_valid)
#test_indices      <- setdiff(not_train_indices, valid_indices)

train_ds <- ds.ltt(df.ltt[, train_indices], extract_elements(true, train_indices))
valid_ds <- ds.ltt(df.ltt[, valid_indices], extract_elements(true, valid_indices))
test_ds  <- ds.ltt(df.ltt[, test_indices] , extract_elements(true, test_indices))


# Creation of the dataloader 
train_dl <- train_ds %>% dataloader(batch_size=batch_size, shuffle=TRUE)
valid_dl <- valid_ds %>% dataloader(batch_size=batch_size, shuffle=FALSE)
test_dl  <- test_ds  %>% dataloader(batch_size=1,          shuffle=FALSE)
```

```{R}

n_hidden  <- 8
n_layer   <- 3
ker_size  <- 5
p_dropout <- 0.01
n_input   <- max_nodes_rounded
n_out     <- length(true)

# Build the CNN

cnn.net <- nn_module(
  
  "corr-cnn",
  
  initialize = function(n_input, n_out, n_hidden, n_layer, ker_size, p_dropout) {
    self$conv1 <- nn_conv1d(in_channels = 1, out_channels = n_hidden, kernel_size = ker_size)
    self$conv2 <- nn_conv1d(in_channels = n_hidden, out_channels = 2*n_hidden, kernel_size = ker_size)
    self$conv3 <- nn_conv1d(in_channels = 2*n_hidden, out_channels = 2*2*n_hidden, kernel_size = ker_size)
    n_flatten <- compute_dim_ouput_flatten_cnn(n_input, n_layer, ker_size)
    self$fc1 <- nn_linear(in_features = n_flatten * (2*2*n_hidden), out_features = 100)
    self$fc2 <- nn_linear(in_features = 100, out_features = n_out)
  },
  
  forward = function(x) {
    x %>% 
      self$conv1() %>%
      nnf_relu() %>%
      nnf_dropout(p = p_dropout) %>%
      nnf_avg_pool1d(2) %>%

      self$conv2() %>%
      nnf_relu() %>%
      nnf_dropout(p = p_dropout) %>%
      nnf_avg_pool1d(2) %>%

      self$conv3() %>%
      nnf_relu() %>%
      nnf_dropout(p = p_dropout) %>%
      nnf_avg_pool1d(2) %>%

      torch_flatten(start_dim = 2) %>%
      self$fc1() %>%
      nnf_relu() %>%
      nnf_dropout(p = p_dropout) %>%
      
      self$fc2()
  }
)


```


```{R}

cnn_ltt <- cnn.net(n_input, n_out, n_hidden, n_layer, ker_size, p_dropout) # create CNN
cnn_ltt$to(device = device) # Move it to the choosen GPU

# Prepare training 

opt <- optim_adam(params = cnn_ltt$parameters) # optimizer 



train_batch <- function(b){
  opt$zero_grad()
  #if (model_type == "crbd"){b$x <- b$x$unsqueeze(2)}
  output <- cnn_ltt(b$x$to(device = device))
  target <- b$y$to(device = device)
  loss <- nnf_mse_loss(output, target)
  loss$backward()
  opt$step()
  loss$item()
}

valid_batch <- function(b) {
  #if (model_type == "crbd"){b$x <- b$x$unsqueeze(2)}
  output <- cnn_ltt(b$x$to(device = device))
  target <- b$y$to(device = device)
  loss <- nnf_mse_loss(output, target)
  loss$item()
}

```

```{R}

# Initialize parameters for the training loop 
epoch     <- 1
trigger   <- 0 
last_loss <- 10000
n_epochs<-100
patience<-10

best_loss<-10000
best_epoch<-0


# Training loop 

train_losses <- list()
valid_losses <- list()
train_plots <- list()
valid_plots <- list()
start_time <-  Sys.time()

while (epoch < n_epochs & trigger < patience) {
  
  # Training 
  cnn_ltt$train()
  train_loss <- c()
  coro::loop(for (b in train_dl) { # loop over batches 
    loss <- train_batch(b)
    train_loss <- c(train_loss, loss)
  })
  
  # Print Epoch and value of Loss function 
  cat(sprintf("epoch %0.3d/%0.3d - train - loss: %3.5f \n",
              epoch, n_epochs, mean(train_loss)))
  
  # Validation 
  cnn_ltt$eval()
  valid_loss <- c()
  coro::loop(for (b in test_dl) { # loop over batches 
    loss <- valid_batch(b)
    valid_loss <- c(valid_loss, loss)
  })
  current_loss <- mean(valid_loss)
  
  # Early Stopping 
  if (current_loss > last_loss){trigger <- trigger + 1 
  
  #print(trigger)
  }else{
    last_loss <- current_loss
    trigger<-0
  }
  
  
  if (current_loss< best_loss){
    #save best model
    torch_save(cnn_ltt, paste("data_PLD/models_pld/PLD_04_CNNLTT","lay",n_layer,"hid",n_hidden,sep="-"))
    best_epoch<-epoch
    best_loss<-current_loss
    
  }
  
  # Print Epoch and value of Loss function
  cat(sprintf("epoch %0.3d/%0.3d - valid - loss: %3.5f \n",
              epoch, n_epochs, current_loss))
  
  train_losses <- c(train_losses, mean(train_loss))
  valid_losses <- c(valid_losses, current_loss)
  
  epoch <- epoch + 1 
  
  
}
end_time <- Sys.time()

print(end_time - start_time)


```
## Loss Plot 
```{R, Loss Trainning vs Validations}

# Plot the loss curve
plot(1:length(train_losses), train_losses, type = "l", col = "blue",
     xlab = "Epoch", ylab = "Loss", main = "Training and Validation Loss",
     ylim = range(c(train_losses, valid_losses)))
lines(1:length(valid_losses), valid_losses, type = "l", col = "red")
legend("topright", legend = c("Training Loss", "Validation Loss"),
       col = c("blue", "red"), lty = 1)


```

## Saving the model




```{R, For only loading the model}


cnn_ltt<-torch_load( paste("data_PLD/models_pld/PLD_04_CNNLTT","lay",n_layer,"hid",n_hidden,sep="-"))
cnn_ltt$to(device=device)

```

```{R}


cnn_ltt$eval()
nn.pred <- vector(mode = "list", length = n_out)
names(nn.pred) <- names(true)

# Compute predictions 
coro::loop(for (b in test_dl) {
  #if (model_type == "crbd"){b$x <- b$x$unsqueeze(2)}
  out <- cnn_ltt(b$x$to(device = device))
  pred <- as.numeric(out$to(device = "cpu")) # move the tensor to CPU 
  #true <- as.numeric(b$y)
  for (i in 1:n_out){nn.pred[[i]] <- c(nn.pred[[i]], pred[i])}
})

```


## Plots Prediction vs Predicted
```{R, Plots Prediciton vs Predicted}
# Prepare plot 
dpi <- 300  # Adjust as needed
width <- 10  # Adjust as needed (in inches)
height <- 5  # Adjust as needed (in inches)

# Start the PNG device with specified parameters
png("Plots/cnnltt_truevspred.png", width = width, height = height, units = "in", res = dpi)

par(mfrow = c(1, 4))
plot(true[[1]][test_indices], nn.pred[[1]], main = "lambda0", xlab = "True", ylab = "Predicted")
abline(0, 1,col="red")

plot(true[[2]][test_indices], nn.pred[[2]], main = "mu", xlab = "True", ylab = "Predicted")
abline(0, 1,col="red")

plot(true[[3]][test_indices] , nn.pred[[3]] , main = "beta_n", xlab = "True", ylab = "Predicted")
abline(0, 1,col="red")

plot(true[[4]][test_indices] , nn.pred[[4]] , main = "beta_p", xlab = "True", ylab = "Predicted")
abline(0, 1,col="red")
dev.off()
```






```{R,computing RMSE}

mse_l <- list()

for (i in 1:length(true))
{
  mse<- sqrt(mean((true[[i]][test_indices]-nn.pred[[i]])^2))/mean(true[[i]][test_indices])
  mse_l<-c(mse_l, mse)
}

names(mse_l) <- names(true)

mse_l


```


```{R,NRME against bucket, Only on testing}

# Get the Nnode values for each tree
node_values <- sapply(phylo[test_indices], function(tree) tree$Nnode)

# Group the indices based on the Nnode values
grouped_indices <- split(seq_along(phylo[test_indices]), node_values)


h <- list()
for( i in 1:length(true)){
  h[[names(true[i])]] <- list()
  
  # Iterate over each group
  for (j in names(grouped_indices)) {
    tree_indices <- grouped_indices[[j]]  # Get the tree indices for the current group
    
    # Get the values for the current name and tree indices
    #values <- true[[i]][tree_indices]
    
    nrmse <- sqrt(mean((true[[i]][test_indices][tree_indices]-nn.pred[[i]][tree_indices])^2))/mean(true[[i]][test_indices][tree_indices])
    
    # Store the values in h
    h[[i]][[j]] <- nrmse
    
    }
  
  
}

```


## Plotting Tree Size vs NRMSE 
```{R, Plotting NRMSE Tree Size }
# Scatter plot for each h[i]
for (i in 1:length(h)) {
  nrmse_values <- unlist(h[[i]])  # Get the NRMSE values for the current h[i]
  
  # Create a scatter plot
  plot(x = as.integer(names(h[[i]])),
       y = nrmse_values,
       xlab = "Number of Nodes in Tree",
       ylab = "NRMSE",
       main = paste("Scatter Plot of NRMSE -", names(h)[i]))
}


```