dd_model = 1 # dd_model = 1 linear dependence in speciation rate with parameter K (= diversity where speciation = extinction)
###Check if tree exists
phylo_name<-fname_ddd(n_trees,lambda0,mu,k,crown_age,dd_model)
if( file.exists( paste("data/phylogeny-", model,"-",phylo_name,".rds",sep=""))==FALSE){
start_time <- Sys.time()
out <- generatePhyloDDD( n_trees,lambda0,mu,k ,age1 = crown_age,ddmodel1=dd_model)
end_time <- Sys.time()
print(end_time - start_time)
phylo <- out$trees
params <- out$param
saveRDS(phylo, paste("data/phylogeny-", model, "-",phylo_name,".rds", sep=""))
saveRDS(params, paste("data/true-param-", model, "-",phylo_name,".rds", sep=""))
#additional tree info
saveRDS(out$tas, paste("data/tas-", model, "-",phylo_name,".rds", sep=""))
saveRDS(out$L, paste("data/L-", model, "-",phylo_name,".rds", sep=""))
saveRDS(out$brts, paste("data/brts-", model, "-",phylo_name,".rds", sep=""))
}else{
print(paste("Tree from model: ",model,", already generated",sep=""))
}
phylo <- readRDS( paste("data/phylogeny-", model, "-",phylo_name,".rds", sep="") ) # change file name if needed
graphs <- generate_phylogeny_graph(phylo)
library(DDD)
library(MLmetrics)
library(dplyr)
library(ape)
library(diversitree)
library(RPANDA)
library(latex2exp)
library(castor)
library(phangorn)
library(svMisc)
library(torch)
library(igraph)
library(scales)
library(extraDistr)
#library(caret)
source("R/infer-general-functions.R")#Contains functions for generating the phylogenetic trees ,plotting, computing rates.
source("R/convert-phylo-to-sumstat.R")#contains the functions to compute the summary statistics of a phylogenetic tree(tips,depth)
source("R/convert-phylo-to-cblv.R") #contains the function to encode a phylogenetic tree with the "Compact Bijective Ladderized
source("R/new_funcs.R")
phylo <- readRDS( paste("data/phylogeny-", model, "-",phylo_name,".rds", sep="") ) # change file name if needed
graphs <- generate_phylogeny_graph(phylo)
phylo <- readRDS( paste("data/phylogeny-", model, "-",phylo_name,".rds", sep="") ) # change file name if needed
graphs <- generate_phylogeny_graph(phylo)
library(DDD)
library(MLmetrics)
library(dplyr)
library(ape)
library(diversitree)
library(RPANDA)
library(latex2exp)
library(castor)
library(phangorn)
library(svMisc)
library(torch)
library(igraph)
library(scales)
library(extraDistr)
#library(caret)
source("R/infer-general-functions.R")#Contains functions for generating the phylogenetic trees ,plotting, computing rates.
source("R/convert-phylo-to-sumstat.R")#contains the functions to compute the summary statistics of a phylogenetic tree(tips,depth)
source("R/convert-phylo-to-cblv.R") #contains the function to encode a phylogenetic tree with the "Compact Bijective Ladderized
source("R/new_funcs.R")
source("R/convert-phylo-to-graph.R")
phylo <- readRDS( paste("data/phylogeny-", model, "-",phylo_name,".rds", sep="") ) # change file name if needed
graphs <- generate_phylogeny_graph(phylo)
saveRDS(graphs, paste("data/phylogeny-",model,"-",phylo_name,"-graph.rds",sep=""))
print("graph info saved")
knitr::opts_chunk$set(echo = TRUE)
library(DDD)
library(MLmetrics)
library(dplyr)
library(ape)
library(diversitree)
library(RPANDA)
library(latex2exp)
library(castor)
library(phangorn)
library(svMisc)
library(torch)
library(igraph)
library(scales)
library(extraDistr)
source("R/infer-general-functions.R")#Contains functions for generating the phylogenetic trees ,plotting, computing rates.
source("R/convert-phylo-to-sumstat.R")#contains the functions to compute the summary statistics of a phylogenetic tree(tips,depth)
source("R/convert-phylo-to-cblv.R") #contains the function to encode a phylogenetic tree with the "Compact Bijective Ladderized
source("R/new_funcs.R")
library(DDD)
library(MLmetrics)
library(dplyr)
library(ape)
library(diversitree)
library(RPANDA)
library(latex2exp)
library(castor)
library(phangorn)
library(svMisc)
library(torch)
library(igraph)
library(scales)
library(extraDistr)
source("R/infer-general-functions.R")#Contains functions for generating the phylogenetic trees ,plotting, computing rates.
source("R/convert-phylo-to-sumstat.R")#contains the functions to compute the summary statistics of a phylogenetic tree(tips,depth)
source("R/convert-phylo-to-cblv.R") #contains the function to encode a phylogenetic tree with the "Compact Bijective Ladderized
source("R/new_funcs.R")
# Set parameters DDD model simulation
set.seed(113)
device = "cpu" # change if you want to compute GPUs
#n_trees <- 100000# number of trees to generate
n_trees <- 10000
model   <- "DDD"
lambda0 <- c(.5, 1.5) # speciation rate
mu      <- c(0.05,0.5) #  extinction rate
k	<-c(40,100)         # carrying Capacity
crown_age <- 15
dd_model = 1 # dd_model = 1 linear dependence in speciation rate with parameter K (= diversity where speciation = extinction)
phylo_name<-fname_ddd(n_trees,lambda0,mu,k,crown_age,dd_model)
if( file.exists( paste("data/phylogeny-",model,"-",phylo_name,"-sumstat.rds",sep="") )==TRUE){
sumstat <- readRDS(paste("data/phylogeny-",model,"-",phylo_name,"-sumstat.rds",sep=""))
true <- readRDS(paste("data/true-param-", model, "-",phylo_name,".rds", sep=""))
true[[3]]<-true[[3]]/100
true[[4]]<-true[[4]]/100
true<-true[c(-4)] # Removing crown age from variables
#Scaling for better
sumstat <- scale_summary_statistics(sumstat, c(100,1000), names(true))
sumstat[87]<-sumstat[87]/100
sumstat[88]<-sumstat[88]/100
sumstat<-sumstat[c(-88)] # Removing crown age from variables
sumstat[]
}else {
print("There is no phylogenetic tree with the previous characteristics please run code 01-DDD_GeneratePhylogeny  ")
}
# Define size of datasets.
total_data_points <- n_trees
subset_size <- 10000  # Specify the size of the subset
n_train    <- floor(subset_size * .9)
n_valid    <- floor(subset_size * .05)
n_test     <- subset_size - n_train - n_valid
batch_size <- subset_size*.01
# Pick the random subset of data points.
subset_indices <- sample(1:total_data_points, subset_size)
# Split the subset into train, validation, and test indices.
train_indices <- subset_indices[1:n_train]
valid_indices <- subset_indices[(n_train + 1):(n_train + n_valid)]
test_indices  <- subset_indices[(n_train + n_valid + 1):subset_size]
# Define size of datasets.
#n_train    <- floor(n_trees * .9)
#n_valid    <- floor(n_trees * .05)
#n_test     <- n_trees - n_train -n_valid
#batch_size <- 1000
set.seed(113)
# Define size of datasets.
total_data_points <- n_trees
subset_size <- 10000  # Specify the size of the subset
n_train    <- floor(subset_size * .9)
n_valid    <- floor(subset_size * .05)
n_test     <- subset_size - n_train - n_valid
batch_size <- min(subset_size*.01, 10)
# Pick the phylogenies randomly.
ds <- convert_ss_dataframe_to_dataset(sumstat)
# Pick the random subset of data points.
subset_indices <- sample(1:total_data_points, subset_size)
# Split the subset into train, validation, and test indices.
train_indices <- subset_indices[1:n_train]
valid_indices <- subset_indices[(n_train + 1):(n_train + n_valid)]
test_indices  <- subset_indices[(n_train + n_valid + 1):subset_size]
#1 train_indices <- sample(1:nrow(sumstat), n_train)
#1 not_train_indices <- setdiff(1:nrow(sumstat), train_indices)
#1 valid_indices <- sample(not_train_indices, n_valid)
#1 test_indices  <- setdiff(not_train_indices, valid_indices)
target_var<-c(names(true))
#target_var<-target_var[-c(3,4)]
# Create the datasets.
train_ds <- ds(sumstat[train_indices, ], target_var, c())
valid_ds <- ds(sumstat[valid_indices, ], target_var, c())
test_ds  <- ds(sumstat[test_indices, ], target_var, c())
# Create the dataloader.
train_dl <- train_ds %>% dataloader(batch_size=batch_size, shuffle=TRUE)
valid_dl <- valid_ds %>% dataloader(batch_size=batch_size, shuffle=FALSE)
test_dl  <- test_ds  %>% dataloader(batch_size=1, shuffle=FALSE)
#test_dl  <- test_ds  %>% dataloader(batch_size=1, shuffle=FALSE)
# Specify neural network parameters.
n_in      <- length(train_ds[1]$x) # number of neurons of the input layer
n_out     <- length(true)
n_hidden  <- 100 # number of neurons in the hidden layers
p_dropout <- 0.01 # dropout probability
n_epochs  <- 100 # maximum number of epochs for the training
patience  <- 10 # patience of the early stopping
# Build the neural network.
dnn.net <- nn_module(
"ss-dnn",
initialize = function(){
self$fc1 <- nn_linear(in_features = n_in, out_features = n_hidden)
self$fc2 <- nn_linear(in_features = n_hidden, out_features = n_hidden)
self$fc3 <- nn_linear(in_features = n_hidden, out_features = n_hidden)
self$fc4 <- nn_linear(in_features = n_hidden, out_features = n_hidden)
self$fc5 <- nn_linear(in_features = n_hidden, out_features = n_out)
},
forward = function(x){
x %>%
self$fc1() %>%
nnf_relu() %>%
nnf_dropout(p = p_dropout) %>%
self$fc2() %>%
nnf_relu() %>%
nnf_dropout(p = p_dropout) %>%
self$fc3() %>%
nnf_relu() %>%
nnf_dropout(p = p_dropout) %>%
self$fc4() %>%
nnf_relu() %>%
nnf_dropout(p = p_dropout) %>%
self$fc5()
}
)
# Set up the neural network.
dnn <- dnn.net() # create CNN
dnn$to(device = device) # Move it to the choosen GPU
opt <- optim_adam(params = dnn$parameters) # optimizer
train_batch <- function(b){
opt$zero_grad()
output <- dnn(b$x$to(device = device))
target <- b$y$to(device = device)
loss <- nnf_mse_loss(output, target)
loss$backward()
opt$step()
loss$item()
}
valid_batch <- function(b) {
output <- dnn(b$x$to(device = device))
target <- b$y$to(device = device)
loss <- nnf_mse_loss(output, target)
loss$item()
}
# Initialize parameters for the training loop.
epoch     <- 1
trigger   <- 0
last_loss <- 10
train_losses <- list()
valid_losses <- list()
train_plots <- list()
valid_plots <- list()
# Training loop.
start_time <-  Sys.time()
while (epoch < n_epochs & trigger < patience) {
# Training
dnn$train()
train_loss <- c()
coro::loop(for (b in train_dl) { # loop over batches
loss <- train_batch(b)
train_loss <- c(train_loss, loss)
})
# Print Epoch and value of Loss function
cat(sprintf("epoch %0.3d/%0.3d - train - loss: %3.5f \n",
epoch, n_epochs, mean(train_loss)))
# Validation
dnn$eval()
valid_loss <- c()
coro::loop(for (b in test_dl) { # loop over batches
loss <- valid_batch(b)
valid_loss <- c(valid_loss, loss)
})
current_loss <- mean(valid_loss)
# Early Stopping
if (current_loss > last_loss){trigger <- trigger + 1}
else{
trigger   <- 0
last_loss <- current_loss
}
# Print Epoch and value of Loss function
cat(sprintf("epoch %0.3d/%0.3d - valid - loss: %3.5f \n",
epoch, n_epochs, current_loss))
train_losses <- c(train_losses, mean(train_loss))
valid_losses <- c(valid_losses, current_loss)
epoch <- epoch + 1
}
device
device
# Set parameters DDD model simulation
set.seed(113)
device = "cuda" # change if you want to compute GPUs
#n_trees <- 100000# number of trees to generate
n_trees <- 10000
model   <- "DDD"
lambda0 <- c(.5, 1.5) # speciation rate
mu      <- c(0.05,0.5) #  extinction rate
k	<-c(40,100)         # carrying Capacity
crown_age <- 15
dd_model = 1 # dd_model = 1 linear dependence in speciation rate with parameter K (= diversity where speciation = extinction)
# Specify neural network parameters.
n_in      <- length(train_ds[1]$x) # number of neurons of the input layer
n_out     <- length(true)
n_hidden  <- 100 # number of neurons in the hidden layers
p_dropout <- 0.01 # dropout probability
n_epochs  <- 100 # maximum number of epochs for the training
patience  <- 10 # patience of the early stopping
# Build the neural network.
dnn.net <- nn_module(
"ss-dnn",
initialize = function(){
self$fc1 <- nn_linear(in_features = n_in, out_features = n_hidden)
self$fc2 <- nn_linear(in_features = n_hidden, out_features = n_hidden)
self$fc3 <- nn_linear(in_features = n_hidden, out_features = n_hidden)
self$fc4 <- nn_linear(in_features = n_hidden, out_features = n_hidden)
self$fc5 <- nn_linear(in_features = n_hidden, out_features = n_out)
},
forward = function(x){
x %>%
self$fc1() %>%
nnf_relu() %>%
nnf_dropout(p = p_dropout) %>%
self$fc2() %>%
nnf_relu() %>%
nnf_dropout(p = p_dropout) %>%
self$fc3() %>%
nnf_relu() %>%
nnf_dropout(p = p_dropout) %>%
self$fc4() %>%
nnf_relu() %>%
nnf_dropout(p = p_dropout) %>%
self$fc5()
}
)
# Set up the neural network.
dnn <- dnn.net() # create CNN
dnn$to(device = device) # Move it to the choosen GPU
opt <- optim_adam(params = dnn$parameters) # optimizer
train_batch <- function(b){
opt$zero_grad()
output <- dnn(b$x$to(device = device))
target <- b$y$to(device = device)
loss <- nnf_mse_loss(output, target)
loss$backward()
opt$step()
loss$item()
}
valid_batch <- function(b) {
output <- dnn(b$x$to(device = device))
target <- b$y$to(device = device)
loss <- nnf_mse_loss(output, target)
loss$item()
}
# Initialize parameters for the training loop.
epoch     <- 1
trigger   <- 0
last_loss <- 10
train_losses <- list()
valid_losses <- list()
train_plots <- list()
valid_plots <- list()
# Training loop.
start_time <-  Sys.time()
while (epoch < n_epochs & trigger < patience) {
# Training
dnn$train()
train_loss <- c()
coro::loop(for (b in train_dl) { # loop over batches
loss <- train_batch(b)
train_loss <- c(train_loss, loss)
})
# Print Epoch and value of Loss function
cat(sprintf("epoch %0.3d/%0.3d - train - loss: %3.5f \n",
epoch, n_epochs, mean(train_loss)))
# Validation
dnn$eval()
valid_loss <- c()
coro::loop(for (b in test_dl) { # loop over batches
loss <- valid_batch(b)
valid_loss <- c(valid_loss, loss)
})
current_loss <- mean(valid_loss)
# Early Stopping
if (current_loss > last_loss){trigger <- trigger + 1}
else{
trigger   <- 0
last_loss <- current_loss
}
# Print Epoch and value of Loss function
cat(sprintf("epoch %0.3d/%0.3d - valid - loss: %3.5f \n",
epoch, n_epochs, current_loss))
train_losses <- c(train_losses, mean(train_loss))
valid_losses <- c(valid_losses, current_loss)
epoch <- epoch + 1
}
end_time <- Sys.time()
print(end_time - start_time)
device
device
torch_tensor(1, device = "cuda")
torch_save(dnn, paste( "M02_DNN-DDD-",subset_size,"Lay","5","Hn",n_hidden,"p",patience,sep="-"))
cat(paste("\n Model rnn saved", sep = ""))
cat("\nSaving model... Done.")
dnn<-torch_load(paste( "M02_DNN-DDD-",subset_size,"Lay","5","Hn",n_hidden,"p",patience,sep="-"))
dnn$eval()
pred <- vector(mode = "list", length = n_out)
names(pred)<-target_var
# Compute predictions
coro::loop(for (b in test_dl) {
out <- dnn(b$x$to(device = device))
p <- as.numeric(out$to(device = "cpu")) # move the tensor to CPU
for (i in 1:n_out){pred[[i]] <- c(pred[[i]], p[i])}
})
dnn<-torch_load(paste( "M02_DNN-DDD-",subset_size,"Lay","5","Hn",n_hidden,"p",patience,sep="-"))
dnn$to(device = device)
dnn$eval()
pred <- vector(mode = "list", length = n_out)
names(pred)<-target_var
# Compute predictions
coro::loop(for (b in test_dl) {
out <- dnn(b$x$to(device = device))
p <- as.numeric(out$to(device = "cpu")) # move the tensor to CPU
for (i in 1:n_out){pred[[i]] <- c(pred[[i]], p[i])}
})
# Prepare plot
par(mfrow = c(1, 3))
plot(true[[1]][test_indices], pred[[1]], main = "lambda0", xlab = "True", ylab = "Predicted")
abline(0, 1,col="red")
plot(true[[2]][test_indices], pred[[2]], main = "mu", xlab = "True", ylab = "Predicted")
abline(0, 1,col="red")
plot(true[[3]][test_indices] * 100, pred[[3]] * 100, main = "K", xlab = "True", ylab = "Predicted")
abline(0, 1,col="red")
mse_l <- list()
for (i in 1:length(true))
{
mse<- sqrt(mean((true[[i]][test_indices]-pred[[i]])^2))/mean(true[[i]][test_indices])
mse_l<-c(mse_l, mse)
}
names(mse_l) <- names(true)
mse_l
phylo_name<-fname_ddd(n_trees,lambda0,mu,k,crown_age,dd_model)
phylo <- readRDS( paste("data/phylogeny-",model,"-",phylo_name,".rds",sep=""))
# Get the Nnode values for each tree
node_values <- sapply(phylo[test_indices], function(tree) tree$Nnode)
# Group the indices based on the Nnode values
grouped_indices <- split(seq_along(phylo[test_indices]), node_values)
h <- list()
for( i in 1:length(true)){
h[[names(true[i])]] <- list()
# Iterate over each group
for (j in names(grouped_indices)) {
tree_indices <- grouped_indices[[j]]  # Get the tree indices for the current group
# Get the values for the current name and tree indices
#values <- true[[i]][tree_indices]
nrmse <- sqrt(mean((true[[i]][test_indices][tree_indices]-pred[[i]][tree_indices])^2))/mean(true[[i]][test_indices][tree_indices])
# Store the values in h
h[[i]][[j]] <- nrmse
}
}
# Scatter plot for each h[i]
for (i in 1:length(h)) {
nrmse_values <- unlist(h[[i]])  # Get the NRMSE values for the current h[i]
# Create a scatter plot
plot(x = as.integer(names(h[[i]])),
y = nrmse_values,
xlab = "Number of Nodes in Tree",
ylab = "NRMSE",
main = paste("Scatter Plot of NRMSE -", names(h)[i]))
}
install.packages("devtools")
install.packages("Rtools")
install.packages(c("cpp11", "DDD"))
library(tools, lib.loc = "C:/Program Files/R/R-4.3.1/library")
devtools::install_github("franciscorichter/NNemesis_Build")
devtools::install_github("franciscorichter/NNemesis_build")
devtools::install_github("franciscorichter/NNemesis")
devtools::install_github("franciscorichter/emphasis")
pkgbuild::check_build_tools(debug = TRUE
)
devtools::install_github("franciscorichter/emphasis")
library(emphasis)
library(emphasis)
devtools::install_github("franciscorichter/emphasis")
library(DDD)
library(MLmetrics)
library(dplyr)
library(ape)
library(diversitree)
library(RPANDA)
library(latex2exp)
library(castor)
library(phangorn)
library(svMisc)
library(torch)
library(igraph)
library(scales)
library(extraDistr)
#library(caret)
source("R/infer-general-functions.R")#Contains functions for generating the phylogenetic trees ,plotting, computing rates.
source("R/convert-phylo-to-sumstat.R")#contains the functions to compute the summary statistics of a phylogenetic tree(tips,depth)
source("R/convert-phylo-to-cblv.R") #contains the function to encode a phylogenetic tree with the "Compact Bijective Ladderized
source("R/convert-phylo-to-graph.R") #contains the function to encode a phylogenetic tree with the "Compact Bijective Ladderized
source("R/new_funcs.R")
# Set parameters DDD model simulation
n_trees   <-  10000# number of trees to generate
ss_check  <- FALSE
source("R/new_funcs.R")
### PLD Simulation
# Set parameters DDD model simulation
## Factor for unitary trees 15
n_trees <- 10000# number of trees to generate
lambda0_pld <- c(0, 50) # speciation rate
k_pld	<- c(20,400)         # carrying Capacity
beta_p <-c(-2.5,2.5)
crown_age_pld <- 1
set.seed(937)
start_time <- Sys.time()
output_pld<-generatePhylo_PLD (n_trees,lambda0_pld,k_pld,beta_p,crown_age_pld,DDD=FALSE
,ss_check=TRUE,max_tries =5)
library(emphasis)
devtools::install_github("franciscorichter/emphasis"
)
setwd("~/Thesis/ML_phylogeny_learning")
